<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <link rel="stylesheet" href="style.css" />
    <title>Nea.chat</title>
  </head>
  <body>
    <h1>V 15</h1>
    <main>
    <button id="button">Start listening</button>
    <p id="message" hidden aria-hidden="true">
      Your browser doesn't support Speech Recognition. Sorry.
    </p>
    <div id="result"></div>
   </main>
    <p>
    <p></p>
    <button id="mic-button">Press to speak</button>

  </body>
  <script>
     window.addEventListener("DOMContentLoaded", () => {
        const button = document.getElementById("button");
        const result = document.getElementById("result");
        const main = document.getElementsByTagName("main")[0];
        const lang = 'de-DE';
        let listening = false;
        const SpeechRecognition =
          window.SpeechRecognition || window.webkitSpeechRecognition;
        if (typeof SpeechRecognition !== "undefined") {
          const recognition = new SpeechRecognition();
          recognition.lang = lang;
          const stop = () => {
            main.classList.remove("speaking");
            recognition.stop();
            button.textContent = "Start listening";
          };

          const start = () => {
            main.classList.add("speaking");
            recognition.start();
            button.textContent = "Stop listening";
          };

          const onResult = event => {
            result.innerHTML = "";
            for (const res of event.results) {
              const text = document.createTextNode(res[0].transcript);
              const p = document.createElement("p");
              if (res.isFinal) {
                p.classList.add("final");
                //completeTranscript(text);
              }
              p.appendChild(text);
              result.appendChild(p);
            }
          };
          recognition.continuous = true;
          recognition.interimResults = false;
          recognition.addEventListener("result", onResult);
          button.addEventListener("click", event => {
            listening ? stop() : start();
            listening = !listening;
          });
        } else {
          button.remove();
          const message = document.getElementById("message");
          message.removeAttribute("hidden");
          message.setAttribute("aria-hidden", "false");
        }
      });



      const micButton = document.getElementById('mic-button');
      micButton.addEventListener('click', startVoiceRecognition);
      var conversation = JSON.parse(localStorage.getItem("history3") || "[]");

      const queryString = window.location.search;
      const urlParams = new URLSearchParams(queryString);
      const API_KEY = urlParams.get('API_KEY');
      function startVoiceRecognition() {
            const recognition = new webkitSpeechRecognition();
            // set the language of the recognition to german
            recognition.lang = lang;
            recognition.onresult = async function (event) {

                const transcript = event.results[0][0].transcript;
                completeTranscript(transcript);
            };
            recognition.start();
      }


      async function completeTranscript(transcript) {
                const input = document.createElement('p');
                input.textContent = transcript;
                document.body.appendChild(input);
                let history = "";
                if (conversation.length > 0) {
                
                   history += "Our conversation so far:\n\n"
                   conversation.forEach(item => {
                    history += `I said: "${item.input}"\n\n`;
                    history += `You responded: "${item.output}"\n\n\n`;
                  });
                 
                } else {
                  console.log("conversation is empty");
                }
                history += `I say: "${transcript}"\n\n`;
                history += `You respond:`;
                
                const completedText = await completeText(history);   
                const output = document.createElement('p');
                output.textContent = completedText;
                document.body.appendChild(output);
                let turn = {
                    input: transcript,
                    output: completedText
                }
                conversation.push(turn);
                console.log(history);
                localStorage.setItem("history3", JSON.stringify(conversation));
  
                // Synthesize speech from the text
                const synth = window.speechSynthesis;
                const speakText = new SpeechSynthesisUtterance(completedText);
                // set the voice to german
                speakText.lang = lang;
                // Get a list of available voices
                const voices = await synth.getVoices();
                // Set the voice to the first available female voice
                speakText.voice = voices.find(voice => voice.gender === 'female');
              
                synth.speak(speakText);
      }

      async function completeText(text) {
          
          var url = "https://api.openai.com/v1/completions";
          var bearer = 'Bearer ' + API_KEY
          const response = await fetch(url, {
            method: 'POST',
            headers: {
              'Authorization': bearer,
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              "prompt": text,
              "model": "text-davinci-003",
              "max_tokens": 1024
            })


          })
            .catch(error => {
              console.log('Something bad happened ' + error)
            });
        data = await response.json();
        return data['choices'][0].text;
         
      }

  </script>
</html>
